{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- The wordlist.txt is still mandatory now, to determine `ix_to_char`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AnacondaPython\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2b9b358dbd7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSimpleRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, TimeDistributed\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b3397eca4e56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"pokemon\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtextfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"wordlists/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\startup-name-generator-master\\utils.py\u001b[0m in \u001b[0;36mtext_to_words\u001b[1;34m(textfile)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtext_to_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext_to_word_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~0123456789–…\\'\\\"’«·»'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from utils import text_to_words\n",
    "\n",
    "# Generate a list of words (including newline)\n",
    "corpus = \"pokemon\"\n",
    "textfile = \"wordlists/\" + corpus + \".txt\"\n",
    "words = text_to_words(textfile)\n",
    "\n",
    "print(words[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8928daf5ab8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Generate the set of unique characters (including newline)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# https://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mchars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate the set of unique characters (including newline)\n",
    "# https://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python\n",
    "chars = sorted(list(set([char for word in words for char in word])))\n",
    "\n",
    "VOCAB_SIZE = len(chars)\n",
    "N_WORDS = len(words)\n",
    "MAX_WORD_LEN = 12  # maximum company name length\n",
    "\n",
    "\n",
    "print(N_WORDS, \"words\\n\")\n",
    "print(\"vocabulary of\", len(chars), \"characters, including the \\\\n:\")\n",
    "print(chars)\n",
    "print(\"\\nFirst two sample words:\")\n",
    "print(words[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((N_WORDS, MAX_WORD_LEN, VOCAB_SIZE))\n",
    "Y = np.zeros((N_WORDS, MAX_WORD_LEN, VOCAB_SIZE))\n",
    "\n",
    "for word_i in range(N_WORDS):\n",
    "    word = words[word_i]\n",
    "    chars = list(word)\n",
    "    word_len = len(word)\n",
    "    \n",
    "    for char_j in range(min(len(word), MAX_WORD_LEN)):\n",
    "        char = chars[char_j]\n",
    "        char_ix = char_to_ix[char]\n",
    "        X[word_i, char_j, char_ix] = 1\n",
    "        if char_j > 0:\n",
    "            Y[word_i, char_j - 1, char_ix] = 1  # the 'next char' at time point char_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_NUM = 2\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, None, 50)          15600     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, None, 50)          20200     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 27)          1377      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, None, 27)          0         \n",
      "=================================================================\n",
      "Total params: 37,177\n",
      "Trainable params: 37,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_scale(probs, temperature = 1.0):\n",
    "    # a low temperature (< 1 and approaching 0) results in the char sampling approaching the argmax.\n",
    "    # a high temperature (> 1, approaching infinity) results in sampling from a uniform distribution)\n",
    "    probs = np.exp(np.log(probs) / temperature)\n",
    "    probs = probs / np.sum(probs)\n",
    "    return probs\n",
    "    \n",
    "    \n",
    "def generate_word(model, temperature = 1.0, min_word_length = 4):\n",
    "    X = np.zeros((1, MAX_WORD_LEN, VOCAB_SIZE))\n",
    "    \n",
    "    # sample the first character\n",
    "    initial_char_distribution = temp_scale(model.predict(X[:, 0:1, :]).flatten(), temperature)\n",
    "    \n",
    "    ix = 0\n",
    "    while ix == 0:  # make sure the initial character is not a newline (i.e. index 0)\n",
    "        ix = int(np.random.choice(VOCAB_SIZE, size = 1, p = initial_char_distribution))\n",
    "    \n",
    "    X[0, 0, ix] = 1\n",
    "    generated_word = [ix_to_char[ix].upper()]  # start with first character, then later successively append chars\n",
    "    \n",
    "    # sample all remaining characters\n",
    "    for i in range(1, MAX_WORD_LEN):\n",
    "        next_char_distribution = temp_scale(model.predict(X[:, 0:i, :])[:, i-1, :].flatten(), temperature)\n",
    "\n",
    "        \n",
    "        ix_choice = np.random.choice(VOCAB_SIZE, size = 1, p = next_char_distribution)\n",
    "        # ix_choice = np.argmax(next_char_distribution)  # <- corresponds to sampling with a very low temperature\n",
    "        ctr = 0\n",
    "        while ix_choice == 0 and i < min_word_length:\n",
    "            ctr += 1\n",
    "            # sample again if you picked the end-of-word token too early\n",
    "            ix_choice = np.random.choice(VOCAB_SIZE, size = 1, p = next_char_distribution)\n",
    "            if ctr > 1000:\n",
    "                print(\"caught in a near-infinite loop. You might have picked too low a temperature and the sampler just keeps sampling \\\\n's\")\n",
    "                break\n",
    "            \n",
    "        \n",
    "        next_ix = int(ix_choice)\n",
    "        X[0, i, next_ix] = 1\n",
    "        if next_ix == 0:\n",
    "            break\n",
    "        generated_word.append(ix_to_char[next_ix])\n",
    "    \n",
    "    return ('').join(generated_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"epoch \" + str(epoch) + \": \" + generate_word(model, temperature = 1.0, min_word_length = 4))\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end = on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Htaz\n",
      "epoch 50: Eklapbk\n",
      "epoch 100: Orimlaxasp\n",
      "epoch 150: Winblis\n",
      "epoch 200: Awostas\n",
      "epoch 250: Miscanel\n",
      "epoch 300: Oleettalllol\n",
      "epoch 350: Ragty\n",
      "epoch 400: Manblate\n",
      "epoch 450: Unna\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 64  # or: N_WORDS\n",
    "\n",
    "model.fit(X, Y, batch_size = BATCH_SIZE, verbose = 0, epochs = NUM_EPOCHS, callbacks = [print_callback])\n",
    "\n",
    "# save the model, but only if the h5 file doesn't exist yet:\n",
    "model_filename = \"models/\" + corpus + \"_\" + str(NUM_EPOCHS) + 'epochs.h5'\n",
    "if not os.path.isfile(model_filename):\n",
    "    model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load one of these models if you have trained them before and want to skip re-training\n",
    "from keras.models import load_model\n",
    "\n",
    "# model = load_model(\"models/behemoth_500epochs.h5\")\n",
    "model = load_model(\"models/pokemon_500epochs.h5\")\n",
    "# model = load_model(\"models/german_500epochs.h5\")\n",
    "# model = load_model(\"models/english_500epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbange\n",
      "Omastar\n",
      "Iniles\n",
      "Avilrash\n",
      "Heritom\n",
      "Nudilor\n",
      "Himomlow\n",
      "Ilgathin\n",
      "Izorua\n",
      "Oursla\n",
      "Regigicash\n",
      "Ordariy\n",
      "Osherdee\n",
      "Origin\n",
      "Ervesacod\n",
      "Alumanol\n",
      "Fryverllas\n",
      "Apisadon\n",
      "Oesseacule\n",
      "Owsing\n",
      "Oyslee\n",
      "Umcoot\n",
      "Izabt\n",
      "Edeveiv\n",
      "Aritoch\n",
      "Encevezezza\n",
      "Edeegtra\n",
      "Edecnukr\n",
      "Odale\n",
      "Hown\n"
     ]
    }
   ],
   "source": [
    "# Print a few words with the final model:\n",
    "\n",
    "for _ in range(30):\n",
    "    print(generate_word(model, temperature = 1, min_word_length = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
